{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NLP Lab Implementation Using Python & NLTK"
      ],
      "metadata": {
        "id": "lg1gkzCHaVgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfKAqdwlaMhW",
        "outputId": "81c35a4c-9992-4a35-beb3-76bb837318d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages if not already installed\n",
        "!pip install nltk scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk, RegexpParser\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n"
      ],
      "metadata": {
        "id": "qQuLCh2QaahP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz87V2OxacoX",
        "outputId": "37edf3bf-4829-4190-e8e2-6e813cc3a9c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization\n",
        "Word Tokenization"
      ],
      "metadata": {
        "id": "XwNX1Hwuafm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"NLTK is a powerful Python library for NLP. It helps process text data easily.\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaY7Yl0Lagqr",
        "outputId": "d10d7f69-502d-4edd-e331-27c1726bf4ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['NLTK', 'is', 'a', 'powerful', 'Python', 'library', 'for', 'NLP', '.', 'It', 'helps', 'process', 'text', 'data', 'easily', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenization"
      ],
      "metadata": {
        "id": "Gaqlp9Qmaiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zdYwwOmalKv",
        "outputId": "adc81d4b-9fc5-40eb-f1df-435e219b1064"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization: ['NLTK is a powerful Python library for NLP.', 'It helps process text data easily.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Stopword Removal"
      ],
      "metadata": {
        "id": "dBrnmqdDapJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
        "print(\"After Stopword Removal:\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXBFCRDvapza",
        "outputId": "df0fdff7-58f4-407a-b9af-48a680722917"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stopword Removal: ['NLTK', 'powerful', 'Python', 'library', 'NLP', 'helps', 'process', 'text', 'data', 'easily']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stemming"
      ],
      "metadata": {
        "id": "mL_5jYf_auka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed_words_porter = [porter.stem(word) for word in filtered_words]\n",
        "print(\"Porter Stemmer:\", stemmed_words_porter)\n",
        "\n",
        "# Using Snowball Stemmer\n",
        "snowball = SnowballStemmer('english')\n",
        "stemmed_words_snowball = [snowball.stem(word) for word in filtered_words]\n",
        "print(\"Snowball Stemmer:\", stemmed_words_snowball)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scn1F9mEavW7",
        "outputId": "85f19c99-2743-4fdc-b81e-2c8e08e83ac9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['nltk', 'power', 'python', 'librari', 'nlp', 'help', 'process', 'text', 'data', 'easili']\n",
            "Snowball Stemmer: ['nltk', 'power', 'python', 'librari', 'nlp', 'help', 'process', 'text', 'data', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Parts of Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "Lu60EMUMazEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_tags = pos_tag(filtered_words)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgehyMTJaz8H",
        "outputId": "79fa8021-8a0b-4001-8f81-1e1fb6555a80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('NLTK', 'NNP'), ('powerful', 'JJ'), ('Python', 'NNP'), ('library', 'NN'), ('NLP', 'NNP'), ('helps', 'VBZ'), ('process', 'VB'), ('text', 'JJ'), ('data', 'NNS'), ('easily', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Lemmatization"
      ],
      "metadata": {
        "id": "3NDf4GgVa4Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to map POS tag to WordNet POS tag\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyCzh30Da5Hy",
        "outputId": "d31ff934-5bf0-4054-d14b-3e163f22c4b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['NLTK', 'powerful', 'Python', 'library', 'NLP', 'help', 'process', 'text', 'data', 'easily']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "tJ467VaVa9Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple NP (Noun Phrase) chunk grammar\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "print(tree)\n",
        "# tree.draw()  # Opens a tree diagram window"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO3Ei7WRa-lW",
        "outputId": "08e726d7-1197-46ab-9fec-24503ff8198e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  NLTK/NNP\n",
            "  powerful/JJ\n",
            "  Python/NNP\n",
            "  (NP library/NN)\n",
            "  NLP/NNP\n",
            "  helps/VBZ\n",
            "  process/VB\n",
            "  text/JJ\n",
            "  data/NNS\n",
            "  easily/RB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "3hKgkQKcbDUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "ner_tree = ne_chunk(pos_tags)\n",
        "print(ner_tree)\n",
        "# ner_tree.draw()  # Opens a tree diagram window for named entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roLQQr5ybBF_",
        "outputId": "d0570d8f-5352-4ccc-ceba-ca7ca89ea4f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  NLTK/NNP\n",
            "  powerful/JJ\n",
            "  (PERSON Python/NNP)\n",
            "  library/NN\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  helps/VBZ\n",
            "  process/VB\n",
            "  text/JJ\n",
            "  data/NNS\n",
            "  easily/RB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. TFâ€“IDF Calculation"
      ],
      "metadata": {
        "id": "4syDbJxNbJC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"NLTK is a powerful Python library for NLP.\",\n",
        "    \"It helps process text data easily.\",\n",
        "    \"TF-IDF is used to find the importance of words in documents.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to array and display\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN0wLvIfbJtn",
        "outputId": "54e8df3b-ce26-4eb6-f095-7b5797bd1018"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       data  documents    easily      find       for     helps       idf  \\\n",
            "0  0.000000   0.000000  0.000000  0.000000  0.389888  0.000000  0.000000   \n",
            "1  0.408248   0.000000  0.408248  0.000000  0.000000  0.408248  0.000000   \n",
            "2  0.000000   0.293884  0.000000  0.293884  0.000000  0.000000  0.293884   \n",
            "\n",
            "   importance        in        is  ...        of  powerful   process  \\\n",
            "0    0.000000  0.000000  0.296520  ...  0.000000  0.389888  0.000000   \n",
            "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.408248   \n",
            "2    0.293884  0.293884  0.223506  ...  0.293884  0.000000  0.000000   \n",
            "\n",
            "     python      text        tf       the        to      used     words  \n",
            "0  0.389888  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.408248  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "2  0.000000  0.000000  0.293884  0.293884  0.293884  0.293884  0.293884  \n",
            "\n",
            "[3 rows x 24 columns]\n"
          ]
        }
      ]
    }
  ]
}